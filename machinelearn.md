# 1.概念

### 1.2.1. 深度学习

深度学习与经典方法的区别主要在于：前者关注的功能强大的模型，这些模型由神经网络错综复杂的交织在一起，包含层层数据转换，因此被称为*深度学习*（deep learning）

### 1.2.2. 目标函数

在机器学习中，我们需要定义模型的优劣程度的度量，这个度量在大多数情况是“可优化”的，我们称之为***目标函数***（objective function） 我们通常定义一个目标函数，并希望优化它到最低点。 因为越低越好，所以这些函数有时被称为***损失函数***（loss function，或cost function）但这只是一个惯例，你也可以取一个新的函数，优化到它的最高点。 这两个函数本质上是相同的，只是翻转一下符号。

当任务在试图预测数值时，最常见的损失函数是***平方误差***（squared error），即预测值与实际值之差的平方。 当试图解决分类问题时，最常见的目标函数是最小化错误率，即预测与实际情况不符的样本比例。 有些目标函数（如平方误差）很容易被优化，有些目标（如错误率）由于不可微性或其他复杂性难以直接优化。 在这些情况下，通常会优化*替代目标*。

通常，损失函数是根据模型参数定义的，并取决于数据集。 在一个数据集上，我们通过最小化总损失来学习模型参数的最佳值。 该数据集由一些为训练而收集的样本组成，称为***训练数据**集*（training dataset，或称为*训练集*（training set））。 然而，在训练数据上表现良好的模型，并不一定在“新数据集”上有同样的效能，这里的“新数据集”通常称为***测试数据集***（test dataset，或称为*测试集*（test set））。当一个模型在训练集上表现良好，但不能推广到测试集时，我们说这个模型是“过拟合”（overfitting）的。

### 1.2.3. 优化算法

一旦我们获得了一些数据源及其表示、一个模型和一个合适的损失函数，我们接下来就需要一种算法，它能够搜索出最佳参数，以最小化损失函数。 深度学习中，大多流行的优化算法通常基于一种基本方法–*梯度下降*（gradient descent）。 简而言之，在每个步骤中，梯度下降法都会检查每个参数，看看如果你仅对该参数进行少量变动，训练集损失会朝哪个方向移动。 然后，它在可以减少损失的方向上优化参数。

### 1.2.4.监督学习

*监督学习*（supervised learning）擅长在“给定输入特征”的情况下预测标签。 每个“特征-标签”对都称为一个*样本*（example）。 有时，即使标签是未知的，样本也可以指代输入特征。 我们的目标是生成一个模型，能够将任何输入特征映射到标签，即预测。

监督学习之所以发挥作用，是因为在训练参数时，我们为模型提供了一个数据集，其中每个样本都有真实的标签。 用概率论术语来说，我们希望预测“估计给定输入特征的标签”的条件概率。 虽然监督学习只是几大类机器学习问题之一，但是在工业中，大部分机器学习的成功应用都是监督学习。 这是因为在一定程度上，许多重要的任务可以清晰地描述为：在给定一组特定的可用数据的情况下，估计未知事物的概率。比如：

- 根据计算机断层扫描（CT）肿瘤图像，预测是否为癌症。
- 给出一个英语句子，预测正确的法语翻译。
- 根据本月的财务报告数据，预测下个月股票的价格。

非正式地说，监督学习的学习过程如下所示。 首先，从已知大量数据样本中随机选取一个子集，为每个样本获取基本的真实标签。 有时，这些样本已有标签（例如，患者是否在下一年内康复？）； 有时，我们可能需要人工标记数据（例如，将图像分类）。 这些输入和相应的标签一起构成了训练数据集。 随后，我们选择有监督的学习算法，它将训练数据集作为输入，并输出一个“完成学习模型”。 最后，我们将之前没见过的样本特征放到这个“完成学习模型”中，使用模型的输出作为相应标签的预测。 整个监督学习过程在 [图1.3.1](https://zh-v2.d2l.ai/chapter_introduction/index.html#fig-supervised-learning) 中绘制。

![../_images/supervised-learning.svg](https://zh-v2.d2l.ai/_images/supervised-learning.svg)

​										*图1.3.1* 监督学习

#### 1.2.4.1回归

***回归***（regression）是最简单的监督学习任务之一。

比方说，假设我们有一组房屋销售数据表格，其中每行对应于每个房子，每列对应于一些相关的属性，例如房屋的面积、卧室的数量、浴室的数量以及到镇中心的步行分钟数等等。 **对机器学习来说，每个样本即为一个特定的房屋，相应的特征向量将是表中的一行。** 如果你住在纽约或旧金山，而且你不是亚马逊、谷歌、微软或Facebook的首席执行官，那么你家中的（平方英尺、卧室数量、浴室数量、步行距离）特征向量可能类似于：[600,1,1,60]。 然而，如果你住在匹兹堡，这个特征向量可能看起来更像[3000,4,3,10]…… 为什么这个任务可以归类为回归问题呢？本质上是**输出决定**的。 假设你在市场上寻找新房子，你可能需要估计一栋房子的公平市场价值。 销售价格，即标签，是一个数值。 当标签取任意数值时，我们称之为*回归*问题。 我们的**目标**是生成一个**模型**，它的预测非常接近实际标签值。

**判断回归问题的一个很好的经验法则是，任何有关“多少”的问题很可能就是回归问题。**比如：

- 这个手术需要多少小时？
- 在未来六小时，这个镇会有多少降雨量？

#### 1.2.4.2.分类

虽然回归模型可以很好地解决“有多少？”的问题，但是很多问题并非如此。这种“哪一个？”的问题叫做*分类*（classification）问题。 在*分类*问题中，我们希望模型能够预测样本属于哪个*类别*（category，正式称为*类*（class））。最简单的分类问题是只有两类，我们称之为“二元分类”。例如，数据集可能由动物图像组成，标签可能是猫狗{猫,狗}两类。 在回归中，我们训练一个回归函数来输出一个数值； 而在分类中，我们训练一个分类器，它的输出即为预测的类别。

然而模型怎么判断得出这种“是”或“不是”的硬分类预测呢？ 我们可以试着用概率语言来理解模型。给定一个样本特征，我们的模型为每个可能的类分配一个概率。 比如，之前的猫狗分类例子中，分类器可能会输出图像是猫的概率为0.9。 0.9这个数字表达什么意思呢？ 我们可以这样解释：分类器90%确定图像描绘的是一只猫。 预测类别的概率的大小传达了一种模型的不确定性，我们将在后面章节中讨论其他运用不确定性概念的算法。

当我们有两个以上的类别时，我们把这个问题称为*多元分类*（multiclass classification）问题。 与解决回归问题不同，分类问题的常见损失函数被称为*交叉熵*（cross-entropy）

分类可能变得比二元分类、多元分类复杂得多。 例如，有一些分类任务的变体可以用于寻找层次结构，层次结构假定在许多类之间存在某种关系。 因此，并不是所有的错误都是均等的。 我们宁愿错误地分入一个相关的类别，也不愿错误地分入一个遥远的类别，这通常被称为***层次分类***(hierarchical classification)。 层次结构相关性可能取决于你计划如何使用模型。

#### 1.2.4.3.标记问题

有些分类问题很适合于二元分类或多元分类。 例如，我们可以训练一个普通的二元分类器来区分猫和狗。 运用最前沿的计算机视觉的算法，我们可以轻松地训练这个模型。 尽管如此，无论我们的模型有多精确，当分类器遇到新的动物时可能会束手无策。 比如这张“不来梅的城市音乐家”的图像 [图1.3.3](https://zh-v2.d2l.ai/chapter_introduction/index.html#fig-stackedanimals) （这是一个流行的德国童话故事），图中有一只猫，一只公鸡，一只狗，一头驴，背景是一些树。 取决于我们最终想用我们的模型做什么，将其视为二元分类问题可能没有多大意义。 取而代之，我们可能想让模型描绘输入图像的内容，一只猫、一只狗、一头驴，还有一只公鸡。 

![../_images/stackedanimals.png](https://zh-v2.d2l.ai/_images/stackedanimals.png)

学习预测不相互排斥的类别的问题称为*多标签分类*（multi-label classification）。

此外，在处理生物医学文献时，我们也会遇到这类问题。 正确地标记文献很重要，有利于研究人员对文献进行详尽的审查。 在国家医学图书馆，一些专业的注释员会检查每一篇在PubMed中被索引的文章，以便将其与Mesh中的相关术语相关联（Mesh是一个大约有28000个标签的集合）。 这是一个十分耗时的过程，注释器通常在归档和标记之间有一年的延迟。 这里，机器学习算法可以提供临时标签，直到每一篇文章都有严格的人工审核。 事实上，近几年来，BioASQ组织已经[举办比赛](http://bioasq.org/)来完成这项工作。

#### 1.2.4.4.搜索

有时，我们不仅仅希望输出为一个类别或一个实值。在信息检索领域，我们希望对一组项目进行排序。 以网络搜索为例，我们的目标不是简单的“查询（query）-网页（page）”分类，而是在海量搜索结果中找到用户最需要的那部分。 搜索结果的排序也十分重要，我们的学习算法需要输出有序的元素子集。 换句话说，如果要求我们输出字母表中的前5个字母，返回“A、B、C、D、E”和“C、A、B、E、D”是不同的。 即使结果集是相同的，集内的顺序有时却很重要。

该问题的一种可能的解决方案：首先为集合中的每个元素分配相应的相关性分数，然后检索评级最高的元素。[PageRank](https://en.wikipedia.org/wiki/PageRank)，谷歌搜索引擎背后最初的秘密武器就是这种评分系统的早期例子，但它的奇特之处在于它不依赖于实际的查询。 在这里，他们依靠一个简单的相关性过滤来识别一组相关条目，然后根据PageRank对包含查询条件的结果进行排序。 如今，搜索引擎使用机器学习和用户行为模型来获取网页相关性得分，很多学术会议也致力于这一主题。

#### 1.2.4.5.推荐系统

另一类与搜索和排名相关的问题是*推荐系统*（recommender system），它的目标是向特定用户进行“个性化”推荐。 例如，对于电影推荐，科幻迷和喜剧爱好者的推荐结果页面可能会有很大不同。 类似的应用也会出现在零售产品、音乐和新闻推荐等等。

尽管推荐系统具有巨大的应用价值，但单纯用它作为预测模型仍存在一些缺陷。 首先，我们的数据只包含“审查后的反馈”：用户更倾向于给他们感觉强烈的事物打分。 例如，在五分制电影评分中，会有许多五星级和一星级评分，但三星级却明显很少。 此外，推荐系统有可能形成反馈循环：推荐系统首先会优先推送一个购买量较大（可能被认为更好）的商品，然而目前用户的购买习惯往往是遵循推荐算法，但学习算法并不总是考虑到这一细节，进而更频繁地被推荐。 综上所述，关于如何处理审查、激励和反馈循环的许多问题，都是重要的开放性研究问题。

#### 1.2.4.6.序列学习

以上大多数问题都具有固定大小的输入和产生固定大小的输出。 例如，在预测房价的问题中，我们考虑从一组固定的特征：平方英尺、卧室数量、浴室数量、步行到市中心的时间； 图像分类问题中，输入为固定尺寸的图像，输出则为固定数量（有关每一个类别）的预测概率； 在这些情况下，模型只会将输入作为生成输出的“原料”，而不会“记住”输入的具体内容。

如果输入的样本之间没有任何关系，以上模型可能完美无缺。 但是如果输入是连续的，我们的模型可能就需要拥有“记忆”功能。 比如，我们该如何处理视频片段呢？ 在这种情况下，每个视频片段可能由不同数量的帧组成。 通过前一帧的图像，我们可能对后一帧中发生的事情更有把握。 语言也是如此，机器翻译的输入和输出都为文字序列。

再比如，在医学上序列输入和输出就更为重要。 设想一下，假设我们用一个模型来监控重症监护病人，如果他们在未来24小时内死亡的风险超过某个阈值，这个模型就会发出警报。 我们绝不希望抛弃过去每小时有关病人病史的所有信息，而仅根据最近的测量结果做出预测。

这些问题是序列学习的实例，是机器学习最令人兴奋的应用之一。 序列学习需要摄取输入序列或预测输出序列，或两者兼而有之。 具体来说，输入和输出都是可变长度的序列，例如机器翻译和从语音中转录文本。 虽然不可能考虑所有类型的序列转换，但以下特殊情况值得一提。

**标记和解析**。这涉及到用属性注释文本序列。 换句话说，输入和输出的数量基本上是相同的。 例如，我们可能想知道动词和主语在哪里，或者，我们可能想知道哪些单词是命名实体。 通常，目标是基于结构和语法假设对文本进行分解和注释，以获得一些注释。 这听起来比实际情况要复杂得多。 下面是一个非常简单的示例，它使用“标记”来注释一个句子，该标记指示哪些单词引用命名实体。 标记为“Ent”，是*实体*（entity）的简写。

```
Tom has dinner in Washington with Sally
Ent  -    -    -     Ent      -    Ent
```

**自动语音识别**。在语音识别中，输入序列是说话人的录音（如 [图1.3.5](https://zh-v2.d2l.ai/chapter_introduction/index.html#fig-speech) 所示），输出序列是说话人所说内容的文本记录。 它的挑战在于，与文本相比，音频帧多得多（声音通常以8kHz或16kHz采样）。 也就是说，音频和文本之间没有1:1的对应关系，因为数千个样本可能对应于一个单独的单词。 这也是“序列到序列”的学习问题，其中输出比输入短得多。

![../_images/speech.png](https://zh-v2.d2l.ai/_images/speech.png)

**文本到语音**。这与自动语音识别相反。 换句话说，输入是文本，输出是音频文件。 在这种情况下，输出比输入长得多。 虽然人类很容易识判断发音别扭的音频文件，但这对计算机来说并不是那么简单。

**机器翻译**。 在语音识别中，输入和输出的出现顺序基本相同。 而在机器翻译中，颠倒输入和输出的顺序非常重要。 换句话说，虽然我们仍将一个序列转换成另一个序列，但是输入和输出的数量以及相应序列的顺序大都不会相同。 比如下面这个例子，“错误的对齐”反应了德国人喜欢把动词放在句尾的特殊倾向。

### 1.2.5 无监督学习

 比如，你的老板可能会给你一大堆数据，然后让你用它做一些数据科学研究，却没有对结果有要求。 我们称这类数据中不含有“目标”的机器学习问题为*无监督学习*（unsupervised learning）那么无监督学习可以回答什么样的问题呢？我们来看看下面的例子：

- *聚类*（clustering）问题：没有标签的情况下，我们是否能给数据分类呢？比如，给定一组照片，我们能把它们分成风景照片、狗、婴儿、猫和山峰的照片吗？同样，给定一组用户的网页浏览记录，我们能否将具有相似行为的用户聚类呢？
- *主成分分析*（principal component analysis）问题：我们能否找到少量的参数来准确地捕捉数据的线性相关属性？比如，一个球的运动轨迹可以用球的速度、直径和质量来描述。再比如，裁缝们已经开发出了一小部分参数，这些参数相当准确地描述了人体的形状，以适应衣服的需要。另一个例子：在欧几里得空间中是否存在一种（任意结构的）对象的表示，使其符号属性能够很好地匹配?这可以用来描述实体及其关系，例如“罗马” − “意大利” + “法国” = “巴黎”。
- *因果关系*（causality）和*概率图模型*（probabilistic graphical models）问题：我们能否描述观察到的许多数据的根本原因？例如，如果我们有关于房价、污染、犯罪、地理位置、教育和工资的人口统计数据，我们能否简单地根据经验数据发现它们之间的关系？
- *生成对抗性网络*（generative adversarial networks）：为我们提供一种合成数据的方法，甚至像图像和音频这样复杂的非结构化数据。潜在的统计机制是检查真实和虚假数据是否相同的测试，它是无监督学习的另一个重要而令人兴奋的领域。

### 1.2.6.与环境互动

机器学习的输入（数据）来自哪里？机器学习的输出又将去往何方？ 到目前为止，不管是监督学习还是无监督学习，我们都会预先获取大量数据，然后启动模型，不再与环境交互。 这里所有学习都是在算法与环境断开后进行的，被称为*离线学习*（offline learning）。 对于监督学习，从环境中收集数据的过程类似于 [图1.3.6](https://zh-v2.d2l.ai/chapter_introduction/index.html#fig-data-collection)。

![image-20221008123422759](G:\python\biji\typoraimage\image-20221008123422759.png)

这种简单的离线学习有它的魅力。 好的一面是，我们可以孤立地进行模式识别，而不必分心于其他问题。 但缺点是，解决的问题相当有限。 如果你更有雄心壮志，那么你可能会期望人工智能不仅能够做出预测，而且能够与真实环境互动。 与预测不同，“与真实环境互动”实际上会影响环境。 这里的人工智能是“智能代理”，而不仅是“预测模型”。 因此，我们必须考虑到它的行为可能会影响未来的观察结果。

当训练和测试数据不同时，最后一个问题提出了*分布偏移*（distribution shift）的问题。

### 1.2.7.强化学习

*深度强化学习*（deep reinforcement learning）将深度学习应用于强化学习的问题，是非常热门的研究领域。 突破性的深度*Q网络*（Q-network）在雅达利游戏中仅使用视觉输入就击败了人类， 以及 AlphaGo 程序在棋盘游戏围棋中击败了世界冠军，是两个突出强化学习的例子。

在强化学习问题中，agent在一系列的时间步骤上与环境交互。 在每个特定时间点，agent从环境接收一些*观察*（observation），并且必须选择一个*动作*（action），然后通过某种机制（有时称为执行器）将其传输回环境，最后agent从环境中获得*奖励*（reward）。 此后新一轮循环开始，agent接收后续观察，并选择后续操作，依此类推。 强化学习的过程在 [图1.3.7](https://zh-v2.d2l.ai/chapter_introduction/index.html#fig-rl-environment) 中进行了说明。 请注意，强化学习的目标是产生一个好的*策略*（policy）。 强化学习agent选择的“动作”受策略控制，即一个从环境观察映射到行动的功能。

![image-20221008123953428](G:\python\biji\typoraimage\image-20221008123953428.png)

强化学习框架的通用性十分强大。 例如，我们可以将任何监督学习问题转化为强化学习问题。

假设我们有一个分类问题，我们可以创建一个强化学习agent，每个分类对应一个“动作”。 然后，我们可以创建一个环境，该环境给予agent的奖励。 这个奖励与原始监督学习问题的损失函数是一致的。

以强化学习在国际象棋的应用为例。 唯一真正的奖励信号出现在游戏结束时：当agent获胜时，agent可以得到奖励1；当agent失败时，agent将得到奖励-1。 因此，强化学习者必须处理*学分分配*（credit assignment）问题：决定哪些行为是值得奖励的，哪些行为是需要惩罚的。 就像一个员工升职一样，这次升职很可能反映了前一年的大量的行动。 要想在未来获得更多的晋升，就需要弄清楚这一过程中哪些行为导致了晋升。

强化学习可能还必须处理部分可观测性问题。 也就是说，当前的观察结果可能无法阐述有关当前状态的所有信息。 比方说，一个清洁机器人发现自己被困在一个许多相同的壁橱的房子里。 推断机器人的精确位置（从而推断其状态），需要在进入壁橱之前考虑它之前的观察结果。

一般的强化学习问题是一个非常普遍的问题。 agent的动作会影响后续的观察，而奖励只与所选的动作相对应。 环境可以是完整观察到的，也可以是部分观察到的,解释所有这些复杂性可能会对研究人员要求太高。 此外，并不是每个实际问题都表现出所有这些复杂性。

当环境可被完全观察到时，我们将强化学习问题称为***马尔可夫决策过程***（markov decision process）。 当状态不依赖于之前的操作时，我们称该问题为***上下文赌博机***（contextual bandit problem）。 当没有状态，只有一组最初未知回报的可用动作时，这个问题就是经典的***多臂赌博机***（multi-armed bandit problem）。

### 1.2.8.关键原则

其核心是当今大多数网络中都可以找到的几个关键原则：

- 线性和非线性处理单元的交替，通常称为*层*（layers）。
- 使用链式规则（也称为*反向传播*（backpropagation））一次性调整网络中的全部参数。

### 1.2.9.特点

机器学习可以使用数据来学习输入和输出之间的转换。 深度学习是“深度”的，模型学习了许多“层”的转换，每一层提供一个层次的表示。 例如，靠近输入的层可以表示数据的低级细节，而接近分类输出的层可以表示用于区分的更抽象的概念。 由于*表示学习*（representation learning）目的是寻找表示本身，因此深度学习可以称为“多级表示学习”。

深度学习方法中最显著的共同点是使用端到端训练。与其基于单独调整的组件组装系统，不如构建系统，然后联合调整它们的性能。

在计算机视觉中，科学家们习惯于将特征工程的过程与建立机器学习模型的过程分开。 Canny边缘检测器 [[Canny, 1987](https://zh-v2.d2l.ai/chapter_references/zreferences.html#id20)] 和SIFT特征提取器 [[Lowe, 2004](https://zh-v2.d2l.ai/chapter_references/zreferences.html#id102)] 作为将图像映射到特征向量的算法，在过去的十年里占据了至高无上的地位。 在过去的日子里，将机器学习应用于这些问题的关键部分是提出人工设计的特征工程方法，将数据转换为某种适合于浅层模型的形式。 然而，与一个算法自动执行的数百万个选择相比，人类通过特征工程所能完成的事情很少。 当深度学习开始时，这些特征抽取器被自动调整的滤波器所取代，产生了更高的精确度。

深度学习的一个关键优势是它不仅取代了传统学习管道末端的浅层模型，而且还取代了劳动密集型的特征工程过程。

除了端到端的训练，我们正在经历从参数统计描述到完全非参数模型的转变。 当数据稀缺时，人们需要依靠简化对现实的假设来获得有用的模型。

接受次优解，处理非凸非线性优化问题，并且愿意在证明之前尝试。

# 2.入门操作



我们介绍n维数组，也称为*张量*（tensor）。 使用过Python中NumPy计算包的读者会对本部分很熟悉。 无论使用哪个深度学习框架，它的*张量类*（在MXNet中为`ndarray`， 在PyTorch和TensorFlow中为`Tensor`）都与Numpy的`ndarray`类似。 但深度学习框架又比Numpy的`ndarray`多一些重要功能： 首先，GPU很好地支持加速计算，而NumPy仅支持CPU计算； 其次，张量类支持自动微分。 这些功能使得张量类更适合深度学习。 如果没有特殊说明，本书中所说的张量均指的是张量类的实例。

学过线性代数的童鞋都知道，矩阵可以进行各种运算，比如：

矩阵的加法：

![image-20221009220434909](G:\python\biji\typoraimage\image-20221009220434909.png)

矩阵的转置

![image-20221009220442838](G:\python\biji\typoraimage\image-20221009220442838.png)

矩阵的点乘：

![image-20221009220453349](G:\python\biji\typoraimage\image-20221009220453349.png)

为了方便存储矩阵及进行矩阵之间的运算，大神们抽象出了PyTorch库，PyTorch库中有一个类叫torch.Tensor，这个类存储了一个矩阵变量，并且有一系列方法用于对这个矩阵进行各种运算。上面的这些矩阵运算都可以通过torch.Tensor类的相应方法实现。

```
import torch  # 引入torch类
x = torch.rand(5, 3)  # 生成一个5*3的矩阵x，矩阵的每个元素都是0到1之间的随机数，x的类型就是torch.Tensor，x里面存了一个5*3的矩阵
y = torch.zeros(5, 3, dtype=torch.long)  # 生成一个5*3的矩阵y，矩阵的每个元素都是0，每个元素的类型是long型，y的类型就是torch.Tensor，y里面存了一个5*3的矩阵
y.add_(x)  # 使用y的add_方法对y和x进行运算，运算结果保存到y
```

通常我们不但要对矩阵进行运算，我们还会对数组进行运算（当然数组也是特殊的矩阵），比如两个数组的加法：

，，，，(1，2，3)+(1，2，3)

在机器学习中，我们更多的会对下面形状这样的数进行运算：

![img](https://pic1.zhimg.com/80/v2-94f7732383f7b46a0c2ec5108e1fe088_720w.webp)

大家可以把这种数看作几个矩阵叠在一起，这种我们暂且给它取一个名字叫“空间矩阵”或者“三维矩阵”。

因此用“矩阵”不能表达所有我们想要进行运算的变量，所以，我们使用张量把矩阵的概念进行扩展。这样普通的矩阵就是二维张量，数组就是一维张量，上面这种空间矩阵就是三维张量，类似的，还有四维、五维、六维张量

那么上面这种三维张量怎么表达呢？

比如对于一维张量[1,2,3]，

在PyTorch中可以表达为torch.tensor([1,2,3])

对于二维张量![image-20221009220915482](G:\python\biji\typoraimage\image-20221009220915482.png)

在PyTorch中可以表达为torch.tensor(**[[1,2,3],[4,5,6]]**)

聪明的读者可能发现了，多一个维度，我们就多加一个**[]**

所以对于上图中的“空间矩阵”，我们可以这样表达：

torch.tensor(**[[[9,1,8],[6,7,5],[3,4,2]],[[2,9,1],[8,6,7],[5,3,4]],[[1,5,9],[7,2,6],[4,8,3]]]**)

```
torch.tensor([
[[9,1,8],[6,7,5],[3,4,2]],
[[2,9,1],[8,6,7],[5,3,4]],
[[1,5,9],[7,2,6],[4,8,3]]
])
```

## 2.1 Torch篇

### 2.1.1 入门

首先，我们导入`torch`。请注意，虽然它被称为PyTorch，但是代码中使用`torch`而不是`pytorch`。

```
import torch
```

![Copy to clipboard](https://raw.githubusercontent.com/choldgraf/sphinx-copybutton/master/sphinx_copybutton/_static/copy-button.svg)

张量表示由一个数值组成的数组，这个数组可能有多个维度。 具有一个轴的张量对应数学上的*向量*（vector）； 具有两个轴的张量对应数学上的*矩阵*（matrix）； 具有两个轴以上的张量没有特殊的数学名称。

首先，我们可以使用 `arange` 创建一个行向量 `x`。这个行向量包含以0开始的前12个整数，它们默认创建为整数。也可指定创建类型为浮点数。张量中的每个值都称为张量的 *元素*（element）。例如，张量 `x` 中有 12 个元素。除非额外指定，新的张量将存储在内存中，并采用基于CPU的计算。

```
x = torch.arange(12)
x
```

可以通过张量的`shape`属性来访问张量（沿每个轴的长度）的*形状* 。

```
x.shape
```

如果只想知道张量中元素的总数，即形状的所有元素乘积，可以检查它的大小（size）。 因为这里在处理的是一个向量，所以它的`shape`与它的`size`相同。

```
x.numel()
```

要想改变一个张量的形状而不改变元素数量和元素值，可以调用`reshape`函数。 例如，可以把张量`x`从形状为（12,）的行向量转换为形状为（3,4）的矩阵。 这个新的张量包含与转换前相同的值，但是它被看成一个3行4列的矩阵。 要重点说明一下，虽然张量的形状发生了改变，但其元素值并没有变。 注意，通过改变张量的形状，张量的大小不会改变。

```
X = x.reshape(3, 4)
X
```

我们不需要通过手动指定每个维度来改变形状。 也就是说，如果我们的目标形状是（高度,宽度）， 那么在知道宽度后，高度会被自动计算得出，不必我们自己做除法。 在上面的例子中，为了获得一个3行的矩阵，我们手动指定了它有3行和4列。 幸运的是，我们可以通过`-1`来调用此自动计算出维度的功能。 即我们可以用`x.reshape(-1,4)`或`x.reshape(3,-1)`来取代`x.reshape(3,4)`。

有时，我们希望使用全0、全1、其他常量，或者从特定分布中随机采样的数字来初始化矩阵。 我们可以创建一个形状为（2,3,4）的张量，其中所有元素都设置为0。

```
torch.zeros((2, 3, 4))
```

同样，我们可以创建一个形状为`(2,3,4)`的张量，其中所有元素都设置为1。代码如下：

```
torch.ones((2, 3, 4))
```

有时我们想通过从某个特定的概率分布中随机采样来得到张量中每个元素的值。 例如，当我们构造数组来作为神经网络中的参数时，我们通常会随机初始化参数的值。 以下代码创建一个形状为（3,4）的张量。 其中的每个元素都从均值为0、标准差为1的标准高斯分布（正态分布）中随机采样。

```
torch.randn(3, 4)
```

我们还可以通过提供包含数值的Python列表（或嵌套列表），来为所需张量中的每个元素赋予确定值。 在这里，最外层的列表对应于轴0，内层的列表对应于轴1。

```
torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
```

```
tensor([[2, 1, 4, 3],
        [1, 2, 3, 4],
        [4, 3, 2, 1]])
```



### 2.1.2. 运算符

```
x1 = torch.tensor([1.0,2,4,8])
y5 = torch.tensor([2,2,2,2])
print(x1+y5)
print(x1*y5)
print(x1/y5)
print(x1**y5)
```

“按元素”方式可以应用更多的计算，包括像求幂这样的一元运算符。

```
torch.exp(x)#指数函数
```

![image-20221009215108523](G:\python\biji\typoraimage\image-20221009215108523.png)

![image-20221009215450425](G:\python\biji\typoraimage\image-20221009215450425.png)

把多个张量*连结*（concatenate）在一起， 把它们端对端地叠起来形成一个更大的张量。 我们只需要提供张量列表，并给出沿哪个轴连结。下面的例子分别演示了当我们沿行（轴-0，形状的第一个元素） 和按列（轴-1，形状的第二个元素）连结两个矩阵时，会发生什么情况。 我们可以看到，第一个输出张量的轴-0长度（6）是两个输入张量轴-0长度的总和（3+3）； 第二个输出张量的轴-1长度（8）是两个输入张量轴-1长度的总和（4+4）。

```
X = torch.arange(12, dtype=torch.float32).reshape((3,4))
Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)
```

![image-20221010213535338](G:\python\biji\typoraimage\image-20221010213535338.png)

有时，我们想通过*逻辑运算符*构建二元张量。 以`X == Y`为例： 对于每个位置，如果`X`和`Y`在该位置相等，则新张量中相应项的值为1。 这意味着逻辑语句`X == Y`在该位置处为真，否则该位置为0。

对张量中的所有元素进行求和，会产生一个单元素张量。

```
X.sum()
```

![image-20221010214605312](G:\python\biji\typoraimage\image-20221010214605312.png)

### 2.1.3. 广播机制

即使形状不同，我们仍然可以通过调用 *广播机制*（broadcasting mechanism）来执行按元素操作。这种机制的工作方式如下：首先，通过适当复制元素来扩展一个或两个数组， 以便在转换之后，两个张量具有相同的形状。 其次，对生成的数组执行按元素操作。在大多数情况下，我们将沿着数组中长度为1的轴进行广播，如下例子：

```
a = torch.arange(3).reshape((3,1))
b = torch.arange(2).reshape((1,2))
print(a)
print(b)
```

由于`a`和`b`分别是3×1和1×2矩阵，如果让它们相加，它们的形状不匹配。 我们将两个矩阵*广播*为一个更大的3×2矩阵，如下所示：矩阵`a`将复制列， 矩阵`b`将复制行，然后再按元素相加

![image-20221010215311803](G:\python\biji\typoraimage\image-20221010215311803.png)

### 2.1.4. 索引和切片

张量中的元素可以通过索引访问,第一个元素的索引是0，最后一个元素索引是-1； 可以指定范围以包含第一个元素和最后一个之前的元素。

如下所示，我们可以用`[-1]`选择最后一个元素，可以用`[1:3]`选择第二个和第三个元素：

![image-20221010215548712](G:\python\biji\typoraimage\image-20221010215548712.png)

除读取外，我们还可以通过指定索引来将元素写入矩阵。

![image-20221010215716586](G:\python\biji\typoraimage\image-20221010215716586.png)

如果我们想为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值。 例如，`[0:2, :]`访问第1行和第2行，其中“:”代表沿轴1（列）的所有元素。 虽然我们讨论的是矩阵的索引，但这也适用于向量和超过2个维度的张量。

![image-20221010215837053](G:\python\biji\typoraimage\image-20221010215837053.png)

### 2.1.5. 节省内存

运行一些操作可能会导致为新结果分配内存。

下面的例子中，我们用Python的`id()`函数演示了这一点， 它给我们提供了内存中引用对象的确切地址。 运行`Y = Y + X`后，我们会发现`id(Y)`指向另一个位置。 这是因为Python首先计算`Y + X`，为结果分配新的内存，然后使`Y`指向内存中的这个新位置。

![image-20221010220937752](G:\python\biji\typoraimage\image-20221010220937752.png)

在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。 通常情况下，我们希望原地执行这些更新。

执行原地操作非常简单。 我们可以使用切片表示法将操作的结果分配给先前分配的数组，例如`Y[:] = <expression>`。 为了说明这一点，我们首先创建一个新的矩阵`Z`，其形状与另一个`Y`相同， 使用`zeros_like`来分配一个全0的块。

![image-20221010222845880](G:\python\biji\typoraimage\image-20221010222845880.png)

如果在后续计算中没有重复使用`X`， 我们也可以使用`X[:] = X + Y`或`X += Y`来减少操作的内存开销。

### 2.1.6. 转换为其他Python对象

将深度学习框架定义的张量转换为NumPy张量（`ndarray`）很容易，反之也同样容易。 torch张量和numpy数组将共享它们的底层内存，就地操作更改一个张量也会同时更改另一个张量。

```
A = X.numpy()
B = torch.tensor(A)
print(type(A))
print(type(B))
```

![image-20221010224728199](G:\python\biji\typoraimage\image-20221010224728199.png)

要将大小为1的张量转换为Python标量，我们可以调用`item`函数或Python的内置函数。

![image-20221010224902997](G:\python\biji\typoraimage\image-20221010224902997.png)

## 2.2. 数据预处理

### 2.2.1. 读取数据集

![image-20221010231855449](G:\python\biji\typoraimage\image-20221010231855449.png)

要从创建的CSV文件中加载原始数据集，我们导入`pandas`包并调用`read_csv`函数。该数据集有四行三列。其中每行描述了房间数量（“NumRooms”）、巷子类型（“Alley”）和房屋价格（“Price”）。

![image-20221010232330576](G:\python\biji\typoraimage\image-20221010232330576.png)

### 2.2.2. 处理缺失值

注意，“NaN”项代表缺失值。 为了处理缺失的数据，典型的方法包括*插值法*和*删除法*， 其中插值法用一个替代值弥补缺失值，而删除法则直接忽略缺失值。 在这里，我们将考虑插值法。

通过位置索引`iloc`，我们将`data`分成`inputs`和`outputs`， 其中前者为`data`的前两列，而后者为`data`的最后一列。 对于`inputs`中缺少的数值，我们用同一列的均值替换“NaN”项。

![image-20221011082616156](G:\python\biji\typoraimage\image-20221011082616156.png)

对于`inputs`中的类别值或离散值，我们将“NaN”视为一个类别。 由于“巷子类型”（“Alley”）列只接受两种类型的类别值“Pave”和“NaN”， `pandas`可以自动将此列转换为两列“Alley_Pave”和“Alley_nan”。 巷子类型为“Pave”的行会将“Alley_Pave”的值设置为1，“Alley_nan”的值设置为0。 缺少巷子类型的行会将“Alley_Pave”和“Alley_nan”分别设置为0和1。

![image-20221011084403675](G:\python\biji\typoraimage\image-20221011084403675.png)

### 2.2.3. 转换为张量格式

现在`inputs`和`outputs`中的所有条目都是数值类型，它们可以转换为张量格式。 当数据采用张量格式后，可以通过在 [2.1节](https://zh-v2.d2l.ai/chapter_preliminaries/ndarray.html#sec-ndarray)中引入的那些张量函数来进一步操作。

![image-20221011085051276](G:\python\biji\typoraimage\image-20221011085051276.png)

## 2.3. 线性代数

### 2.3.1.向量长度、维度和形状

向量的长度通常称为向量的*维度*,我们可以通过调用Python的内置`len()`函数来访问张量的长度。

当用张量表示一个向量（只有一个轴）时，我们也可以通过`.shape`属性访问向量的长度。 形状（shape）是一个元素组，列出了张量沿每个轴的长度（维数）。 对于只有一个轴的张量，形状只有一个元素。

### 2.3.2.矩阵

 当矩阵具有相同数量的行和列时，其形状将变为正方形； 因此，它被称为*方阵*（square matrix）。

当调用函数来实例化张量时， 我们可以通过指定两个分量m和n来创建一个形状为m×n的矩阵。

```
A = torch.arange(20).reshape(5, 4)
print(A)
```

当我们交换矩阵的行和列时，结果称为矩阵的*转置*（transpose）。 我们用a⊤来表示矩阵的转置，如果B=A⊤， 则对于任意i和j，都有bij=aji。 因此，在 [(2.3.2)](https://zh-v2.d2l.ai/chapter_preliminaries/linear-algebra.html#equation-eq-matrix-def)中的转置是一个形状为n×m的矩阵：

```
A.T
```

作为方阵的一种特殊类型，*对称矩阵*（symmetric matrix）A等于其转置：A=A⊤。 这里我们定义一个对称矩阵B：

```
B = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])
print(B)
```

现在我们将`B`与它的转置进行比较。

```
print(B == B.T)
```

![image-20221011090949180](G:\python\biji\typoraimage\image-20221011090949180.png)

### 2.3.3.张量

将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。

![image-20221011092554903](G:\python\biji\typoraimage\image-20221011092554903.png)

### 2.3.4.降维









































